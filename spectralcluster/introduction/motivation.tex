\subsection{Motivation}

Our new method of generating clustering problems is inspired by two
separate lines of work, which both pioneer the use of variational
methods for
classical machine learning problems: the first is the work of von
Luxburg, Belkin, and Bosquet~\cite{von2008consistency} on determining how 
spectral clusering algorithms of Ng, Jordan, and
Weiss~\cite{NgSpectral01} (over 10,000 citations) behave as the number
of samples grows large. This was refined by Trillos and
Slepcev~\cite{TrillosRate15, TrillosVariational15}.
The second line of work models optimization algorithms like gradient descent
as continuous-time differential equations. The latter method was introduced
by Su, Boyd, and Candes in~\cite{SuNesterov14} and expanded upon by
Wibisono, Wilson, and Jordan in ~\cite{WibisonoGradient16}.


The first line of work on spectral clustering advances the idea that
clustering should be seen as a discrete analog of a partitioning process
on an underlying probability density. The work of Trillos and
Slepcev~\cite{TrillosVariational15} proves that the
spectral clustering methods of Ng, Jordan, and Weiss converge to what we
will call a $(1,2)$ spectral sweep cut of a probability density
function, as the number of samples grows large. We will define these
sweep cuts formally in Section~\ref{sec:density-contribution}. We note
that Trillos and Slepcev
make certain regularity assumptions on the probability density (such as
being bounded above and below) in order to guarantee their results. For
readers interested in seeing the extent of these assumptions, we refer
them to their paper at~\cite{TrillosVariational15}.

Our work differs from the work of Trillos and Slepcev, and the spectral
clustering algorithm of Ng et al, in a significant way: first, while
Trillos and Slepcev proved that the
spectral clustering algorithm of Ng. et al converge to a $(1,2)$ spectral
sweep cut of the underlying probability density as the number of samples
grow large, there are currently no guarantees on the sparsity of such
cuts.
In our work, we propose instead to cluster
using a $(1,3)$ spectral sweep cut on the probability density. We will show that such a sweep cut has
desirable sparse-cut or isoperimetry properties on any $L$-Lipschitz
probability density, for appropriately
defined notions of cut sparsity. We will also show that $(1,2)$ spectral
sweep cuts cannot have nice sparse-cut guarantees.

Meanwhile, the work of Su, Boyd, and Candes~\cite{SuNesterov14} involves taking a
discrete optimization algorithm, such as accelerated gradient descent, and
finding a continuous time algorithm that this descent method converges
to given extremely small step sizes.  This approach was expanded
upon by Wibisono, Wilson, and Jordan~\cite{WibisonoGradient16}, where they generated new
gradient descent methods by first looking at continuous descent
processes with desirable properties,
and then writing down discretizations. This strategy of first looking at
continuous algorithms and then discretizing them, inspires our new
strategy for generating clustering algorithms.

These two past works motivate our clustering strategy: first, we find
desirable partitionings for continuous probability density functions, and then we
write down discretizations. In the rest of this introduction, we
formally define our notions of sparsity and spectral sweep cuts,
write down our core theorems, discuss past work, and carefully enumerate
our proof and theoretical contributions.

