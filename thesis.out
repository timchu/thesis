\BOOKMARK [0][-]{chapter.1}{1 Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{1.1 Quick Preliminaries \(Clustering Definitions and Geometry in ML\)}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{1.2 Paper Overview}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.3}{1.3 Group Theory in Kernels and Natural Language Processing}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.4}{1.4 Data-Sensitive Distances in Clustering}{chapter.1}% 5
\BOOKMARK [1][-]{section.1.5}{1.5 Spectral Clustering in Large Datasets}{chapter.1}% 6
\BOOKMARK [1][-]{section.1.6}{1.6 Spectral Graph Theory in Machine Learning}{chapter.1}% 7
\BOOKMARK [0][-]{chapter.2}{2 Metric Embeddings and Group Theory in Kernels and Natural Language Processing}{}% 8
\BOOKMARK [1][-]{section.2.1}{2.1 Introduction}{chapter.2}% 9
\BOOKMARK [2][-]{subsection.2.1.1}{2.1.1 Kernel Methods}{section.2.1}% 10
\BOOKMARK [2][-]{subsection.2.1.2}{2.1.2 Metric Transforms}{section.2.1}% 11
\BOOKMARK [2][-]{subsection.2.1.3}{2.1.3 Only Polynomials Preserve Low Rank Matrices}{section.2.1}% 12
\BOOKMARK [1][-]{section.2.2}{2.2 Technique Overview}{chapter.2}% 13
\BOOKMARK [2][-]{subsection.2.2.1}{2.2.1 Key Lemma}{section.2.2}% 14
\BOOKMARK [2][-]{subsection.2.2.2}{2.2.2 Kernel Methods}{section.2.2}% 15
\BOOKMARK [2][-]{subsection.2.2.3}{2.2.3 Metric Transforms}{section.2.2}% 16
\BOOKMARK [2][-]{subsection.2.2.4}{2.2.4 Polynomial Method Converse}{section.2.2}% 17
\BOOKMARK [1][-]{section.2.1}{A Preliminaries}{chapter.2}% 18
\BOOKMARK [2][-]{subsection.2.1.1}{A.1 Notations}{section.2.1}% 19
\BOOKMARK [2][-]{subsection.2.1.2}{A.2 Definitions}{section.2.1}% 20
\BOOKMARK [2][-]{subsection.2.1.3}{A.3 Alternate Classifications of Completely Monotone and Bernstein Functions}{section.2.1}% 21
\BOOKMARK [2][-]{subsection.2.1.4}{A.4 Metric Hierarchies}{section.2.1}% 22
\BOOKMARK [2][-]{subsection.2.1.5}{A.5 Negative Type Metrics and Euclidean Embeddability}{section.2.1}% 23
\BOOKMARK [2][-]{subsection.2.1.6}{A.6 Useful Tools}{section.2.1}% 24
\BOOKMARK [1][-]{section.2.2}{B Non-Polynomial Functions Blow Up Matrix Rank}{chapter.2}% 25
\BOOKMARK [2][-]{subsection.2.2.1}{B.1 Preliminaries}{section.2.2}% 26
\BOOKMARK [2][-]{subsection.2.2.2}{B.2 One Eigenvalue is Identically Zero}{section.2.2}% 27
\BOOKMARK [2][-]{subsection.2.2.3}{B.3 Only Polynomials Have a Zero Eigenvalue}{section.2.2}% 28
\BOOKMARK [2][-]{subsection.2.2.4}{B.4 Rewriting the Sum}{section.2.2}% 29
\BOOKMARK [2][-]{subsection.2.2.5}{B.5 Calculating the Limit}{section.2.2}% 30
\BOOKMARK [2][-]{subsection.2.2.6}{B.6 Main Result}{section.2.2}% 31
\BOOKMARK [1][-]{section.2.3}{C Transforming Manhattan to Euclidean}{chapter.2}% 32
\BOOKMARK [2][-]{subsection.2.3.1}{C.1 Useful Computations}{section.2.3}% 33
\BOOKMARK [2][-]{subsection.2.3.2}{C.2 Main Results}{section.2.3}% 34
\BOOKMARK [2][-]{subsection.2.3.3}{C.3 Function Should be Bounded}{section.2.3}% 35
\BOOKMARK [1][-]{section.2.4}{D Transforming Manhattan to Manhattan}{chapter.2}% 36
\BOOKMARK [2][-]{subsection.2.4.1}{D.1 Useful Tools}{section.2.4}% 37
\BOOKMARK [2][-]{subsection.2.4.2}{D.2 Main Result}{section.2.4}% 38
\BOOKMARK [2][-]{subsection.2.4.3}{D.3 Discussion and Extensions}{section.2.4}% 39
\BOOKMARK [1][-]{section.2.5}{E Positive Definite Manhattan Kernels}{chapter.2}% 40
\BOOKMARK [2][-]{subsection.2.5.1}{E.1 A Useful Tool}{section.2.5}% 41
\BOOKMARK [2][-]{subsection.2.5.2}{E.2 Main Result}{section.2.5}% 42
\BOOKMARK [1][-]{section.2.6}{F Representation Theory of the Real Hyperrectangles}{chapter.2}% 43
\BOOKMARK [2][-]{subsection.2.6.1}{F.1 Useful Tools}{section.2.6}% 44
\BOOKMARK [2][-]{subsection.2.6.2}{F.2 Main Result}{section.2.6}% 45
\BOOKMARK [0][-]{chapter.3}{3 Data-Sensitive Distances}{}% 46
\BOOKMARK [1][-]{section.3.1}{1 Introduction}{chapter.3}% 47
\BOOKMARK [2][-]{subsection.3.1.1}{1.1 Contributions and Past Work}{section.3.1}% 48
\BOOKMARK [2][-]{subsection.3.1.2}{1.2 Definitions and Preliminaries}{section.3.1}% 49
\BOOKMARK [1][-]{section.3.2}{2 Outline}{chapter.3}% 50
\BOOKMARK [1][-]{section.3.3}{3 Exactly Computing the nearest neighbor metric}{chapter.3}% 51
\BOOKMARK [2][-]{subsection.3.3.1}{3.1 From Finite Sets to Finite Collections of Compact Path-Connected Bodies}{section.3.3}% 52
\BOOKMARK [1][-]{section.3.4}{4 Persistent Homology of the Nearest-neighbor Geodesic Distance}{chapter.3}% 53
\BOOKMARK [1][-]{section.3.5}{5 Relating the nearest neighbor metric to Euclidean MSTs, Euclidean Spanners, and More}{chapter.3}% 54
\BOOKMARK [2][-]{subsection.3.5.1}{5.1 Relation to the Euclidean MST problem}{section.3.5}% 55
\BOOKMARK [2][-]{subsection.3.5.2}{5.2 Generalizing Single Linkage Clustering, Level Sets, and k-Centers clustering}{section.3.5}% 56
\BOOKMARK [1][-]{section.3.6}{6 Spanners for the nearest neighbor metric}{chapter.3}% 57
\BOOKMARK [2][-]{subsection.3.6.1}{6.1 Exact-spanners of nearest neighbor metric in the Probability Density Setting }{section.3.6}% 58
\BOOKMARK [2][-]{subsection.3.6.2}{6.2 Fast, Sparse Spanner for the Edge-Squared Metric}{section.3.6}% 59
\BOOKMARK [1][-]{section.3.7}{7 Conclusions and Open Questions}{chapter.3}% 60
\BOOKMARK [1][-]{section.3.7}{G Nearest Neighbor Metric and Edge-Power Metrics relate to Single Linkage Clustering, Level Sets, and k-Centers clustering}{chapter.3}% 61
\BOOKMARK [1][-]{section.3.8}{H Proving Faster and Sparser-than-Euclidean Approximate Spanners}{chapter.3}% 62
\BOOKMARK [2][-]{subsection.3.8.1}{H.1 1+O\(2\) spanners can be generated from a 1/ WSPD}{section.3.8}% 63
\BOOKMARK [1][-]{section.3.9}{I Spanners in the Probability Density Setting: Full Proof}{chapter.3}% 64
\BOOKMARK [0][-]{chapter.4}{4 Spectral Clustering in the Limit}{}% 65
\BOOKMARK [1][-]{section.4.1}{A Introduction}{chapter.4}% 66
\BOOKMARK [2][-]{subsection.4.1.1}{A.1 Definitions}{section.4.1}% 67
\BOOKMARK [2][-]{subsection.4.1.2}{A.2 Theorems}{section.4.1}% 68
\BOOKMARK [2][-]{subsection.4.1.3}{A.3 Past Work}{section.4.1}% 69
\BOOKMARK [1][-]{section.4.2}{B Paper Organization}{chapter.4}% 70
\BOOKMARK [1][-]{section.4.3}{C Cheeger-Buser inequalities require carefully chosen , , }{chapter.4}% 71
\BOOKMARK [1][-]{section.4.4}{D Buser Inequality for Probability Density Functions}{chapter.4}% 72
\BOOKMARK [2][-]{subsection.4.4.1}{D.1 Weighted Buser-type Inequality}{section.4.4}% 73
\BOOKMARK [2][-]{subsection.4.4.2}{D.2 Proof Strategy: Mollification by Disks of Radius Proportional to rho}{section.4.4}% 74
\BOOKMARK [2][-]{subsection.4.4.3}{D.3 Key Technical Lemma: Bounding L1 norm of a function with the L1 norm of its mollification}{section.4.4}% 75
\BOOKMARK [2][-]{subsection.4.4.4}{D.4 Upper Bounding the Numerator}{section.4.4}% 76
\BOOKMARK [2][-]{subsection.4.4.5}{D.5 Lower Bound on the Denominator}{section.4.4}% 77
\BOOKMARK [2][-]{subsection.4.4.6}{D.6 Bounding the Rayleigh Quotient \(Proof of Theorem D.4\)}{section.4.4}% 78
\BOOKMARK [2][-]{subsection.4.4.7}{D.7 Gradient of Mollifier}{section.4.4}% 79
\BOOKMARK [2][-]{subsection.4.4.8}{D.8 Scaling}{section.4.4}% 80
\BOOKMARK [1][-]{section.4.5}{E Cheeger Inequality for Probability Density Functions}{chapter.4}% 81
\BOOKMARK [1][-]{section.4.6}{F Spectral Sweep Cuts have Provably Good Sparsity \(proof of Theorem A.8\)}{chapter.4}% 82
\BOOKMARK [1][-]{section.4.7}{G Problems with Existing Spectral Cut Methods}{chapter.4}% 83
\BOOKMARK [2][-]{subsection.4.7.1}{G.1 Our density function}{section.4.7}% 84
\BOOKMARK [2][-]{subsection.4.7.2}{G.2 Proof Overview}{section.4.7}% 85
\BOOKMARK [2][-]{subsection.4.7.3}{G.3 The Zero-set of a principal \(1,2\) eigenfunction is the line y = 0}{section.4.7}% 86
\BOOKMARK [2][-]{subsection.4.7.4}{G.4 Any spectral sweep cut has high \(1,\) sparsity}{section.4.7}% 87
\BOOKMARK [1][-]{section.4.8}{H Conclusion and Future Directions}{chapter.4}% 88
\BOOKMARK [1][-]{section.4.7}{G Calculating Eigenvalues and Isoperimetry constants for Simple Examples}{chapter.4}% 89
\BOOKMARK [2][-]{subsection.4.7.1}{G.1 Notation}{section.4.7}% 90
\BOOKMARK [2][-]{subsection.4.7.2}{G.2 A Lipschitz weight}{section.4.7}% 91
\BOOKMARK [1][-]{section.4.8}{H Cheeger and Buser for Density Functions does not easily follow from Graph or Manifold Cheeger and Buser}{chapter.4}% 92
\BOOKMARK [2][-]{subsection.4.8.1}{H.1 Comments on Graph Cheeger-Buser}{section.4.8}% 93
\BOOKMARK [2][-]{subsection.4.8.2}{H.2 Comments on Manifold Cheeger-Buser}{section.4.8}% 94
\BOOKMARK [1][-]{section.4.9}{I A weighted Cheeger inequality in one dimension}{chapter.4}% 95
\BOOKMARK [0][-]{chapter.5}{5 Geometric Spectral Algorithms and Hardness, with Machine Learning applications}{}% 96
\BOOKMARK [1][-]{section.5.1}{1 Introduction}{chapter.5}% 97
\BOOKMARK [2][-]{subsection.5.1.1}{1.1 High-dimensional results}{section.5.1}% 98
\BOOKMARK [2][-]{subsection.5.1.2}{1.2 Our Techniques}{section.5.1}% 99
\BOOKMARK [2][-]{subsection.5.1.3}{1.3 Brief summary of our results in terms of }{section.5.1}% 100
\BOOKMARK [2][-]{subsection.5.1.4}{1.4 Summary of our Results on Examples}{section.5.1}% 101
\BOOKMARK [2][-]{subsection.5.1.5}{1.5 Other Related Work}{section.5.1}% 102
\BOOKMARK [1][-]{section.5.2}{2 Summary of Low Dimensional Results}{chapter.5}% 103
\BOOKMARK [2][-]{subsection.5.2.1}{2.1 Multiplication}{section.5.2}% 104
\BOOKMARK [2][-]{subsection.5.2.2}{2.2 Sparsification}{section.5.2}% 105
\BOOKMARK [2][-]{subsection.5.2.3}{2.3 Laplacian solving}{section.5.2}% 106
\BOOKMARK [1][-]{section.5.3}{3 Preliminaries}{chapter.5}% 107
\BOOKMARK [2][-]{subsection.5.3.1}{3.1 Notation}{section.5.3}% 108
\BOOKMARK [2][-]{subsection.5.3.2}{3.2 Graph and Laplacian Notation}{section.5.3}% 109
\BOOKMARK [2][-]{subsection.5.3.3}{3.3 Spectral Sparsification via Random Sampling}{section.5.3}% 110
\BOOKMARK [2][-]{subsection.5.3.4}{3.4 Woodbury Identity}{section.5.3}% 111
\BOOKMARK [2][-]{subsection.5.3.5}{3.5 Tail Bounds}{section.5.3}% 112
\BOOKMARK [2][-]{subsection.5.3.6}{3.6 Fine-Grained Hypotheses}{section.5.3}% 113
\BOOKMARK [2][-]{subsection.5.3.7}{3.7 Dimensionality Reduction}{section.5.3}% 114
\BOOKMARK [2][-]{subsection.5.3.8}{3.8 Nearest Neighbor Search}{section.5.3}% 115
\BOOKMARK [2][-]{subsection.5.3.9}{3.9 Geometric Laplacian System}{section.5.3}% 116
\BOOKMARK [1][-]{section.5.4}{4 Equivalence of Matrix-Vector Multiplication and Solving Linear Systems}{chapter.5}% 117
\BOOKMARK [2][-]{subsection.5.4.1}{4.1 Solving Linear Systems Implies Matrix-Vector Multiplication}{section.5.4}% 118
\BOOKMARK [2][-]{subsection.5.4.2}{4.2 Matrix-Vector Multiplication Implies Solving Linear Systems}{section.5.4}% 119
\BOOKMARK [2][-]{subsection.5.4.3}{4.3 Lower bound for high-dimensional linear system solving}{section.5.4}% 120
\BOOKMARK [1][-]{section.5.5}{5 Matrix-Vector Multiplication}{chapter.5}% 121
\BOOKMARK [2][-]{subsection.5.5.1}{5.1 Equivalence between Adjacency and Laplacian Evaluation}{section.5.5}% 122
\BOOKMARK [2][-]{subsection.5.5.2}{5.2 Approximate Degree}{section.5.5}% 123
\BOOKMARK [2][-]{subsection.5.5.3}{5.3 `Kernel Method' Algorithms}{section.5.5}% 124
\BOOKMARK [2][-]{subsection.5.5.4}{5.4 Lower Bound in High Dimensions}{section.5.5}% 125
\BOOKMARK [2][-]{subsection.5.5.5}{5.5 Lower Bounds in Low Dimensions}{section.5.5}% 126
\BOOKMARK [2][-]{subsection.5.5.6}{5.6 Hardness of the -Body Problem}{section.5.5}% 127
\BOOKMARK [2][-]{subsection.5.5.7}{5.7 Hardness of Kernel PCA}{section.5.5}% 128
\BOOKMARK [1][-]{section.5.6}{6 Sparsifying Multiplicatively Lipschitz Functions in Almost Linear Time}{chapter.5}% 129
\BOOKMARK [2][-]{subsection.5.6.1}{6.1 High Dimensional Sparsification}{section.5.6}% 130
\BOOKMARK [2][-]{subsection.5.6.2}{6.2 Low Dimensional Sparsification}{section.5.6}% 131
\BOOKMARK [1][-]{section.5.7}{7 Sparsifiers for }{chapter.5}% 132
\BOOKMARK [2][-]{subsection.5.7.1}{7.1 Existence of large expanders in inner product graphs}{section.5.7}% 133
\BOOKMARK [2][-]{subsection.5.7.2}{7.2 Efficient algorithm for finding sets with low effective resistance diameter}{section.5.7}% 134
\BOOKMARK [2][-]{subsection.5.7.3}{7.3 Using low-effective-resistance clusters to sparsify the unweighted IP graph}{section.5.7}% 135
\BOOKMARK [2][-]{subsection.5.7.4}{7.4 Sampling data structure}{section.5.7}% 136
\BOOKMARK [2][-]{subsection.5.7.5}{7.5 Weighted IP graph sparsification}{section.5.7}% 137
\BOOKMARK [1][-]{section.5.8}{8 Hardness of Sparsifying and Solving Non-Multiplicatively-Lipschitz Laplacians}{chapter.5}% 138
\BOOKMARK [1][-]{section.5.9}{9 Fast Multipole Method}{chapter.5}% 139
\BOOKMARK [2][-]{subsection.5.9.1}{9.1 Overview}{section.5.9}% 140
\BOOKMARK [2][-]{subsection.5.9.2}{9.2 , Fast Gaussian transform}{section.5.9}% 141
\BOOKMARK [2][-]{subsection.5.9.3}{9.3 Generalization}{section.5.9}% 142
\BOOKMARK [2][-]{subsection.5.9.4}{9.4 }{section.5.9}% 143
\BOOKMARK [1][-]{section.5.10}{10 Neural Tangent Kernel}{chapter.5}% 144
\BOOKMARK [0][-]{chapter.6}{6 Conclusion}{}% 145
